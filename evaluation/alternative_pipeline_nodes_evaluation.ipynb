{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ce31844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import openai\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from matplotlib.patches import Rectangle\n",
    "from pyvis.network import Network\n",
    "from pyvis.network import Network\n",
    "from gingerit.gingerit import GingerIt\n",
    "from rapidfuzz.distance import Levenshtein\n",
    "\n",
    "from transformers import AutoImageProcessor, AutoModelForObjectDetection\n",
    "from PIL import Image, ImageDraw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee135bb8",
   "metadata": {},
   "source": [
    "### One function for each step in the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb3d1da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ocr_data(json_file):\n",
    "    with open(json_file) as fp:\n",
    "        data = json.loads(fp.read())\n",
    "    \n",
    "    nodes = []\n",
    "\n",
    "    for b in data['Blocks']:\n",
    "        if b['BlockType'] == 'LINE' and (len(b['Text']) > 2):\n",
    "            node = {'text': b['Text'], \n",
    "                    'left': b['Geometry']['BoundingBox']['Left'], \n",
    "                    'top': b['Geometry']['BoundingBox']['Top'],\n",
    "                    'right': b['Geometry']['BoundingBox']['Left'] + b['Geometry']['BoundingBox']['Width'],\n",
    "                    'bottom': b['Geometry']['BoundingBox']['Top'] + b['Geometry']['BoundingBox']['Height']}\n",
    "\n",
    "            nodes.append(node)\n",
    "    \n",
    "    return pd.DataFrame(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cf03d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_image(filename):\n",
    "    \n",
    "    img = cv2.imread(filename)\n",
    "    \n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aec08276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_image(image):\n",
    "    \n",
    "    img = image.copy()\n",
    "    \n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    mean_tone_value = np.mean(gray)\n",
    "    \n",
    "    #print(mean_tone_value)\n",
    "    \n",
    "    if mean_tone_value < 128:\n",
    "        \n",
    "        gray = 255 - gray\n",
    "        mean_tone_value = np.mean(gray)\n",
    "    \n",
    "    threshold_value = int(mean_tone_value * 0.8)\n",
    "    \n",
    "    _, threshold = cv2.threshold(gray, threshold_value, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    threshold = 1 - (threshold / 255.)\n",
    "    \n",
    "    return threshold\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad2c2783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_bounding_boxes_in_pixels(df, img):\n",
    "    \n",
    "    img_height = img.shape[0]\n",
    "    img_width = img.shape[1]\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "         \n",
    "        df.at[i, 'left']   = int(round(row['left'] * img_width))\n",
    "        df.at[i, 'right']  = int(round(row['right'] * img_width))\n",
    "        df.at[i, 'top']    = int(round(row['top'] * img_height))\n",
    "        df.at[i, 'bottom'] = int(round(row['bottom'] * img_height))\n",
    "        \n",
    "    df['left']   = df['left'].astype(int)\n",
    "    df['right']  = df['right'].astype(int)\n",
    "    df['top']    = df['top'].astype(int)\n",
    "    df['bottom'] = df['bottom'].astype(int)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86bb7df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_font_size(df):\n",
    "    \n",
    "    df['font_size'] = df.bottom - df.top\n",
    "    \n",
    "    df['font_size'] = (df['font_size'] - df['font_size'].mean()) / (df['font_size'].std() + 1e-6)\n",
    "    \n",
    "    df['font_size'] = (df['font_size'].apply(lambda x: round(x)) + 10).astype(int)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2135a370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def substract_bounding_boxes(df, img, erotion_percent = 0):\n",
    "    \n",
    "    img_out = img.copy()\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        \n",
    "        width = row['right'] - row['left']\n",
    "        erotion_width = int(round((width * erotion_percent) / 100))\n",
    "        \n",
    "        height = row['bottom'] - row['top']\n",
    "        erotion_height = int(round((height * erotion_percent) / 100))\n",
    "        \n",
    "\n",
    "        img_out[ (row['top'] + erotion_height)  : (row['bottom'] - erotion_height), \n",
    "                 (row['left'] + erotion_width) : (row['right'] - erotion_width) ] = 0\n",
    "    \n",
    "    return img_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "282dd41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def close_shape_gaps5(image, ocr,\n",
    "                      dist_threshold_percent = 30, \n",
    "                      activation_lower_th = 40, \n",
    "                      activation_upper_th = 70):\n",
    "\n",
    "    img = image.copy()\n",
    "    img = (1-img) * 10\n",
    "\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    kernel[1,1] = 10\n",
    "\n",
    "    dst = cv2.filter2D(img,-1,kernel).astype(int)\n",
    "\n",
    "    points_thr = np.where((dst > activation_lower_th) & (dst < activation_upper_th))\n",
    "\n",
    "    points = []\n",
    "    for p_i in range(len(points_thr[0])): \n",
    "        points.append([points_thr[0][p_i], points_thr[1][p_i]])\n",
    "\n",
    "    points = np.stack(points, axis=0)\n",
    "\n",
    "    nodes_points = []\n",
    "\n",
    "    nodes_points.extend([[row.top, row.left] for i, row in ocr.iterrows()])\n",
    "    nodes_points.extend([[row.top, row.right] for i, row in ocr.iterrows()])\n",
    "    nodes_points.extend([[row.bottom, row.right] for i, row in ocr.iterrows()])\n",
    "    nodes_points.extend([[row.bottom, row.left] for i, row in ocr.iterrows()])\n",
    "\n",
    "    nodes_points   = np.array(nodes_points)\n",
    "    dist_matrix    = euclidean_distances(points)\n",
    "    max_bb_height  = (ocr.bottom - ocr.top).max()\n",
    "    dist_threshold = int((max_bb_height * dist_threshold_percent)/100)\n",
    "\n",
    "    below_th = np.where((dist_matrix < dist_threshold) & (dist_matrix > 0)) # zero is trivial distance, no need to fill any gap\n",
    "\n",
    "    img_out = image.copy()\n",
    "\n",
    "    for i in range(len(below_th[0])):\n",
    "\n",
    "        p1 = points[below_th[0][i]]\n",
    "        p2 = points[below_th[1][i]]\n",
    "\n",
    "        dist_to_nodes = euclidean_distances(np.stack([p1, p2]), nodes_points)\n",
    "        closest_node = np.argmin(dist_to_nodes) % len(ocr)\n",
    "\n",
    "        closest_node_height = ocr.loc[closest_node, 'bottom'] - ocr.loc[closest_node, 'top']\n",
    "\n",
    "        dist_threshold = int((closest_node_height * dist_threshold_percent)/100)\n",
    "\n",
    "        if np.linalg.norm(p2-p1) < dist_threshold:\n",
    "\n",
    "            cv2.line(img_out, [p1[1],p1[0]], [p2[1],p2[0]],  (1, 1, 1), thickness=1)\n",
    "    \n",
    "    return img_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d19b59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stamp_bounding_boxes_on_image(df, img, erotion_percent = 10):\n",
    "    \n",
    "    img_out = img.copy()\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        \n",
    "        width = row['right'] - row['left']\n",
    "        erotion_width = int(round((width * erotion_percent) / 100))\n",
    "        \n",
    "        height = row['bottom'] - row['top']\n",
    "        erotion_height = int(round((height * erotion_percent) / 100))\n",
    "        \n",
    "\n",
    "        img_out[ (row['top'] + erotion_height)  : (row['bottom'] - erotion_height), \n",
    "                 (row['left'] + erotion_width) : (row['right'] - erotion_width) ] = 1\n",
    "    \n",
    "    return img_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c8c3ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filled_shapes(img):\n",
    "    \n",
    "    contours, tree = cv2.findContours(cv2.convertScaleAbs(img), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    img_out = np.zeros_like(img)\n",
    "\n",
    "    for i, contour in enumerate(contours):\n",
    "        cv2.drawContours(img_out, [contour], 0, (1, 1, 1), thickness=cv2.FILLED)\n",
    "        \n",
    "    return img_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4ac9481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masks(img, max_iter=10):\n",
    "    \n",
    "\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "\n",
    "    img_eroded = [img.copy()]\n",
    "    contours_iter = []\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        contours, tree = cv2.findContours(cv2.convertScaleAbs(img_eroded[-1]), \n",
    "                                          cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        contours_iter.append(contours)\n",
    "        img_eroded.append(cv2.erode(img_eroded[-1], kernel, iterations = 1))\n",
    "    \n",
    "    min_contours = len(contours_iter[-1])\n",
    "    min_contours_iteration = len(contours_iter)-1\n",
    "\n",
    "    for i in range(len(contours_iter)-1, -1, -1):\n",
    "        if len(contours_iter[i]) > min_contours:\n",
    "            min_contours_iteration = i+1\n",
    "            break\n",
    "            \n",
    "            \n",
    "    nodes_mask = img_eroded[min_contours_iteration]\n",
    "    \n",
    "    nodes_mask_dilated = cv2.dilate(nodes_mask, kernel, iterations=min_contours_iteration+1)\n",
    "    edges_mask = np.maximum((img_eroded[0] - nodes_mask_dilated), 0)\n",
    "\n",
    "\n",
    "    return nodes_mask, edges_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "602ebf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edges_endpoints(edges_mask, min_edge_length_percentage = 3):\n",
    "    \n",
    "    final_edges = []\n",
    "    \n",
    "    min_edge_length_pixels = (min_edge_length_percentage / 100) * edges_mask.shape[0]\n",
    "\n",
    "    contours, tree = cv2.findContours(cv2.convertScaleAbs(edges_mask), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    for contour in contours:\n",
    "        \n",
    "        c = max([contour], key=cv2.contourArea)\n",
    "        \n",
    "        extreme_points = []\n",
    "\n",
    "        extreme_points.append(np.array(c[c[:, :, 0].argmin()][0]))\n",
    "        extreme_points.append(np.array(c[c[:, :, 0].argmax()][0]))\n",
    "        extreme_points.append(np.array(c[c[:, :, 1].argmin()][0]))\n",
    "        extreme_points.append(np.array(c[c[:, :, 1].argmax()][0]))\n",
    "        \n",
    "        extreme_points = np.stack(extreme_points, axis=0)\n",
    "\n",
    "        dist_mat = euclidean_distances(extreme_points)\n",
    "        \n",
    "        if np.max(dist_mat) > min_edge_length_pixels:\n",
    "        \n",
    "            ext_indeces = np.unravel_index(np.argmax(dist_mat), shape=dist_mat.shape)\n",
    "\n",
    "\n",
    "            final_endpoints = [extreme_points[ext_indeces[0]], extreme_points[ext_indeces[1]]]\n",
    "\n",
    "            final_edges.append(final_endpoints)\n",
    "\n",
    "\n",
    "    return np.stack(final_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8f1fff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nodes(ocr, nodes_mask, threshold_iou = 0.8):\n",
    "    \n",
    "    df = ocr.copy()\n",
    "    nodes_contours, tree = cv2.findContours(cv2.convertScaleAbs(nodes_mask), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "\n",
    "        area = (row['right'] - row['left']) * (row['bottom'] - row['top'])\n",
    "\n",
    "        max_iou = 0\n",
    "        max_iou_i_node = -1\n",
    "        \n",
    "\n",
    "        for i_node, contour in enumerate(nodes_contours):\n",
    "\n",
    "            empty_img = np.zeros_like(nodes_mask)\n",
    "\n",
    "            cv2.drawContours(empty_img, [contour], 0, (1, 1, 1), thickness=-1)\n",
    "\n",
    "            intersection = empty_img[row['top']:row['bottom'], row['left']:row['right']].sum()\n",
    "\n",
    "            iou = intersection / area\n",
    "\n",
    "            if iou > max_iou:\n",
    "                max_iou = iou\n",
    "                max_iou_i_node = i_node\n",
    "\n",
    "        if max_iou > threshold_iou:\n",
    "\n",
    "            df.at[i, 'node_id'] = max_iou_i_node\n",
    "            \n",
    "    df['text'] = df.groupby('node_id')['text'].transform(lambda x: '\\n'.join(x))\n",
    "    df.drop_duplicates('text', inplace=True)\n",
    "\n",
    "    df =df[df.node_id.notna()]\n",
    "    df.node_id = df.node_id.astype(int)\n",
    "    \n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8832eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conections(nodes_df, edges_endpoints, img, dist_threshold_percentage = 5):\n",
    "    \n",
    "    nodes_contours = []\n",
    "    nodes_ids = []\n",
    "    \n",
    "    for i, row in nodes_df.iterrows():\n",
    "        \n",
    "        img_out = np.zeros_like(img, dtype=np.uint16)\n",
    "        \n",
    "        img_out[ row.top : row.bottom, row.left : row.right ] = 1\n",
    "        \n",
    "        contour, tree = cv2.findContours(cv2.convertScaleAbs(img_out), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        assert len(contour) == 1\n",
    "        \n",
    "        nodes_contours.append(contour[0])\n",
    "        nodes_ids.append(row.node_id)\n",
    "        \n",
    "    \n",
    "    \n",
    "    edges_endpoints = edges_endpoints.astype(np.uint16)\n",
    "\n",
    "    connections = []\n",
    "\n",
    "    dist_threshold_in_pixels = int((dist_threshold_percentage / 100) * img.shape[0])\n",
    "\n",
    "    for edge in edges_endpoints:\n",
    "\n",
    "        connection = [None, None]\n",
    "\n",
    "        for i_endpoint, endpoint in enumerate(edge):\n",
    "\n",
    "            min_dist_to_node = 9e3\n",
    "            min_dist_node_n = -1\n",
    "\n",
    "            for i_node, node in enumerate(nodes_contours):\n",
    "\n",
    "                min_dist = cv2.pointPolygonTest(node, endpoint, True) * (-1)\n",
    "\n",
    "\n",
    "                if min_dist < min_dist_to_node:\n",
    "                    min_dist_to_node = min_dist\n",
    "                    min_dist_node_n = nodes_ids[i_node]\n",
    "\n",
    "            if min_dist_to_node < dist_threshold_in_pixels:\n",
    "\n",
    "                connection[i_endpoint] = min_dist_node_n\n",
    "\n",
    "        if connection[0] is not None and connection[1] is not None and connection[0] != connection[1]:\n",
    "            connections.append(connection) \n",
    "\n",
    "    df = pd.DataFrame(connections, columns=['node a', 'node b'])\n",
    "\n",
    "    froze_set = set([frozenset([row['node a'], row['node b']]) for i, row in df.iterrows()])\n",
    "\n",
    "    df = pd.DataFrame(froze_set, columns=['node a', 'node b'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7dd8717d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_image_nodes_from_annotated_df(df):\n",
    "    \n",
    "    df = df[df.text.apply(lambda x: x.startswith('image') == False)]\n",
    "    \n",
    "    if 'type' in df.columns:\n",
    "        return df[df.type != 'image']\n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "373059a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_close_nodes(ocr, vertical_distance_threshold_percent=25, horizontal_distance_threshold_percent=50):\n",
    "    \n",
    "    df = ocr.copy()\n",
    "    \n",
    "    while True:\n",
    "\n",
    "        to_add = []\n",
    "        to_remove = []\n",
    "\n",
    "        flag_updates_made = False\n",
    "\n",
    "        for i,row_a in df.iterrows():\n",
    "            for j,row_b in df.iterrows():\n",
    "\n",
    "                row_a_height = row_a.bottom - row_a.top\n",
    "                row_b_height = row_b.bottom - row_b.top\n",
    "\n",
    "                row_a_width = row_a.right - row_a.left\n",
    "                row_b_width = row_b.right - row_b.left\n",
    "\n",
    "                mean_height = (row_a_height + row_b_height) / 2\n",
    "                mean_width  = (row_a_width + row_b_width) / 2\n",
    "\n",
    "                vertical_distance_threshold_pixels   = (vertical_distance_threshold_percent / 100) * mean_height\n",
    "                horizontal_distance_threshold_pixels = (horizontal_distance_threshold_percent / 100) * mean_width\n",
    "\n",
    "                if (j > i and \n",
    "                    abs(row_b.top - row_a.bottom) < vertical_distance_threshold_pixels and\n",
    "                    abs(row_b.left - row_a.left) < horizontal_distance_threshold_pixels):\n",
    "                    \n",
    "                    df.at[i, 'text'] = row_a.text + ' ' + row_b.text\n",
    "                    df.at[i, 'bottom'] = row_b.bottom\n",
    "                    df.at[i, 'left'] = min(row_a.left, row_b.left)\n",
    "                    df.at[i, 'right'] = max(row_a.right, row_b.right)\n",
    "                    df.at[i, 'font_size'] = (row_a.font_size + row_b.font_size) / 2\n",
    "\n",
    "                    df = df.drop(j, axis=0)\n",
    "                    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "                    flag_updates_made = True\n",
    "                   \n",
    "                    break\n",
    "\n",
    "            if flag_updates_made:\n",
    "                break\n",
    "        \n",
    "        if flag_updates_made == False:\n",
    "            break\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f071db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = GingerIt()\n",
    "\n",
    "def spellcheck2(text):\n",
    "    text = text.replace('&', 'and')\n",
    "    res = parser.parse(text)\n",
    "    output = res['result']\n",
    "    output = output.replace(' and ', ' & ')\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1938fc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_nodes_with_closest_annotations(pred_df, annotated_df, dist_threshold = 0.35):\n",
    "    \n",
    "    predicted_df = pred_df.copy()\n",
    "    \n",
    "    predicted_df.insert(column='closest_ann_text', loc=1, value=None)\n",
    "    predicted_df.insert(column='closest_ann_node_id', loc=len(predicted_df.columns), value=-1)\n",
    "    predicted_df.insert(column='closest_ann_dist', loc=len(predicted_df.columns), value=1)\n",
    "    \n",
    "    for i, row_a in predicted_df.iterrows():\n",
    "        \n",
    "        min_dist = dist_threshold\n",
    "        min_dist_text = None\n",
    "        min_dist_node_id = -1\n",
    "        \n",
    "        for j, row_b in annotated_df.iterrows():\n",
    "            \n",
    "            dist = Levenshtein.normalized_distance(row_a.text, row_b.text)\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                min_dist_text = row_b.text\n",
    "                min_dist_node_id = row_b.node_id\n",
    "        \n",
    "        predicted_df.at[i, 'closest_ann_text']    = min_dist_text\n",
    "        predicted_df.at[i, 'closest_ann_node_id'] = min_dist_node_id\n",
    "        predicted_df.at[i, 'closest_ann_dist']    = min_dist\n",
    "    \n",
    "    predicted_df = predicted_df[predicted_df.closest_ann_dist < dist_threshold]\n",
    "    \n",
    "    return predicted_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caf13db",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c991071",
   "metadata": {},
   "source": [
    "## functions to calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e2d743e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nodes_metrics(ann_nodes, nodes_df):\n",
    "    \n",
    "    nodes_df = pair_nodes_with_closest_annotations(nodes_df.copy(), ann_nodes)\n",
    "    \n",
    "    predictions = set(nodes_df.closest_ann_text.apply(lambda x: x.lower()).tolist())\n",
    "    annotations = set(ann_nodes.text.apply(lambda x: x.lower()).tolist())\n",
    "    \n",
    "    tp = annotations & predictions\n",
    "    \n",
    "    precision = len(tp) / (len(predictions) + 1e-6)\n",
    "    recall    = len(tp) / (len(annotations) + 1e-6)\n",
    "    f1        = (2 * precision * recall) / (precision+recall + 1e-6)\n",
    "    \n",
    "    return f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "606ed7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(nodes_f1, nodes_pr, nodes_re):\n",
    "    \n",
    "    sns.set_theme()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,4))\n",
    "\n",
    "    ax.plot(range(1,len(nodes_f1)+1), nodes_f1, label='F1')\n",
    "    ax.plot(range(1,len(nodes_pr)+1), nodes_pr, label='Precision')\n",
    "    ax.plot(range(1,len(nodes_re)+1), nodes_re, label='Recall')\n",
    "    ax.set_ylim(0,1)\n",
    "    ax.set_title('Node retrieval')\n",
    "    ax.legend()\n",
    "\n",
    "    print(f'Mean F1-score for node retrieval: {np.mean(nodes_f1)}')\n",
    "\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f09f31",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e13e102",
   "metadata": {},
   "source": [
    "## get all annotated examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5204da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of annotated examples: 21\n"
     ]
    }
   ],
   "source": [
    "main_folder = '../annotated_examples'\n",
    "\n",
    "ann_examples = sorted(os.listdir(main_folder))\n",
    "\n",
    "n_examples = len(ann_examples)\n",
    "\n",
    "print(f'number of annotated examples: {n_examples}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d1d93ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_exclude_to_time_complexity = ['Hipshot for inflation',\n",
    "                                 'IMG_0300-1664016398.2217-scaled',\n",
    "                                 'Mathematics Map',\n",
    "                                 'erythrocytes L1_2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197aedce",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20458fd1",
   "metadata": {},
   "source": [
    "# new baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0474ea4c-56a8-4aec-8e1b-e203cb0cd054",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_checkpoint = \"models/node\"\n",
    "\n",
    "node_image_processor = AutoImageProcessor.from_pretrained(node_checkpoint)\n",
    "node_model = AutoModelForObjectDetection.from_pretrained(node_checkpoint)\n",
    "\n",
    "\n",
    "def get_node_predictions(image, threshold=0.1):\n",
    "\n",
    "    img     = Image.fromarray(image)\n",
    "    inputs  = node_image_processor(images=img, return_tensors=\"pt\")\n",
    "    outputs = node_model(**inputs)\n",
    "    \n",
    "    target_sizes = torch.tensor([img.size[::-1]])\n",
    "    return node_image_processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=threshold)[0]\n",
    "# def get_node_predictions(image, threshold=0.1):\n",
    "\n",
    "#     img     = Image.fromarray(image)\n",
    "#     inputs  = node_image_processor(images=img, return_tensors=\"pt\")\n",
    "#     outputs = node_model(**inputs)\n",
    "    \n",
    "#     target_sizes = torch.tensor([img.size[::-1]])\n",
    "#     results = node_image_processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=threshold)[0] \n",
    "#     draw = ImageDraw.Draw(image)\n",
    "#     for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "#         box = [round(i, 2) for i in box.tolist()]\n",
    "#         print(\n",
    "#                 f\"Detected {model.config.id2label[label.item()]} with confidence \"\n",
    "#                 f\"{round(score.item(), 3)} at location {box}\"\n",
    "#         )\n",
    "#         draw.rectangle(box, outline=\"red\", width=1)\n",
    "    \n",
    "#     return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4439026c-67d8-4a40-a88a-940591843e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(annotations_file, images_folder):\n",
    "    \n",
    "    with open(annotations_file) as fp:\n",
    "        annotations = json.loads(fp.read())\n",
    "    \n",
    "    elements = []\n",
    "    \n",
    "    for i, image_description in enumerate(annotations['images']):\n",
    "    \n",
    "        img = Image.open(os.path.join(images_folder, image_description['file_name']))\n",
    "    \n",
    "        objects = {'id':[], \n",
    "                   'area': [],\n",
    "                   'bbox': [],\n",
    "                   'category': []}\n",
    "        \n",
    "        for j, ann_description in enumerate(annotations['annotations']):\n",
    "    \n",
    "            if ann_description['image_id'] == image_description['id']:\n",
    "    \n",
    "                objects['id'].append(ann_description['id'])\n",
    "                objects['area'].append(ann_description['area'])\n",
    "                objects['bbox'].append(ann_description['bbox'])\n",
    "                objects['category'].append(ann_description['category_id'])\n",
    "        \n",
    "        \n",
    "        el = {'image_id': image_description['id'],\n",
    "              'image': img,\n",
    "              'width': image_description['width'],\n",
    "              'height': image_description['height'],\n",
    "              'objects': objects\n",
    "        }\n",
    "    \n",
    "        elements.append(el)\n",
    "\n",
    "    return Dataset.from_list(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16ac2b04-365c-4ffd-bef8-353f2e97b79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(box1, box2):\n",
    "    \"\"\"Compute Intersection over Union of two boxes.\"\"\"\n",
    "    x1, y1, x2, y2 = box1\n",
    "    x1_, y1_, x2_, y2_ = box2\n",
    "    \n",
    "    xi1, yi1, xi2, yi2 = max(x1, x1_), max(y1, y1_), min(x2, x2_), min(y2, y2_)\n",
    "    \n",
    "    intersection = max(xi2 - xi1, 0) * max(yi2 - yi1, 0)\n",
    "    \n",
    "    area_box1 = (x2 - x1) * (y2 - y1)\n",
    "    area_box2 = (x2_ - x1_) * (y2_ - y1_)\n",
    "    \n",
    "    union = area_box1 + area_box2 - intersection\n",
    "    \n",
    "    return intersection / union if union > 0 else 0\n",
    "\n",
    "def non_maximum_suppression(detections, iou_threshold=0.5):\n",
    "    \"\"\"Apply NMS on detections and return filtered detections.\"\"\"\n",
    "    detections = sorted(detections, key=lambda x: x['Confidence Score'], reverse=True)\n",
    "    keep = []\n",
    "    while detections:\n",
    "        max_score_det = detections.pop(0)\n",
    "        keep.append(max_score_det)\n",
    "        detections = [\n",
    "            det for det in detections\n",
    "            if compute_iou(max_score_det['Location'], det['Location']) < iou_threshold\n",
    "        ]\n",
    "    return keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "111f43d2-019b-4c42-8e61-04500a73180f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def show_predictions(image, threshold=0.9):\n",
    "    \n",
    "    inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    target_sizes = torch.tensor([image.size[::-1]])\n",
    "    results = image_processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=threshold)[0]\n",
    "    \n",
    "    draw = ImageDraw.Draw(image)\n",
    "    \n",
    "    # List to collect detection details\n",
    "    detections = []\n",
    "    for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "        box = [round(i, 2) for i in box.tolist()]\n",
    "        detection_info = {\n",
    "            'Label': model.config.id2label[label.item()],\n",
    "            'Confidence Score': round(score.item(), 3),\n",
    "            'Location': box\n",
    "        }\n",
    "        detections.append(detection_info)\n",
    "        \n",
    "        # print(\n",
    "        #     f\"Detected {detection_info['Label']} with confidence \"\n",
    "        #     f\"{detection_info['Confidence Score']} at location {detection_info['Location']}\"\n",
    "        # )\n",
    "        # draw.rectangle(box, outline=\"red\", width=1)\n",
    "\n",
    "    \n",
    "    # Convert the list of dictionaries to a DataFrame and save as a CSV file\n",
    "    detections_df = pd.DataFrame(detections)\n",
    "\n",
    "    return image, detections_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0bdae234-4990-44f2-b193-7d06e4f58533",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "test_images_folder = 'data/real_data/real_test_nodes/'\n",
    "test_annotations_file = 'data/real_data/real_test_nodes/result.json'\n",
    "image_processor = AutoImageProcessor.from_pretrained(node_checkpoint)\n",
    "model = AutoModelForObjectDetection.from_pretrained(node_checkpoint)\n",
    "\n",
    "dataset_test = get_datasets(test_annotations_file, test_images_folder)\n",
    "# image_test,df_test =  show_predictions(dataset_test[13]['image'], 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c7b85fc-13d4-4e3b-924d-ff7a066e39ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image_id \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m13\u001b[39m, \u001b[38;5;241m20\u001b[39m]:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m precision, recall \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_precision_recall_for_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m all_precisions\u001b[38;5;241m.\u001b[39mappend(precision)\n\u001b[0;32m     35\u001b[0m all_recalls\u001b[38;5;241m.\u001b[39mappend(recall)\n",
      "Cell \u001b[1;32mIn[30], line 3\u001b[0m, in \u001b[0;36mcompute_precision_recall_for_image\u001b[1;34m(image_id, threshold)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_precision_recall_for_image\u001b[39m(image_id, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     gt_boxes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mround\u001b[39m, ann[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbox\u001b[39m\u001b[38;5;124m\"\u001b[39m])) \u001b[38;5;28;01mfor\u001b[39;00m ann \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdata\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mannotations\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m ann[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m image_id]\n\u001b[0;32m      4\u001b[0m     image_test, df_test \u001b[38;5;241m=\u001b[39m show_predictions(dataset_test[image_id][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m], threshold)\n\u001b[0;32m      6\u001b[0m     pred_boxes \u001b[38;5;241m=\u001b[39m df_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLocation\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "def compute_precision_recall_for_image(image_id, threshold=0.9):\n",
    "    # \n",
    "    gt_boxes = [list(map(round, ann[\"bbox\"])) for ann in data[\"annotations\"] if ann[\"image_id\"] == image_id]\n",
    "    image_test, df_test = show_predictions(dataset_test[image_id]['image'], threshold)\n",
    "\n",
    "    pred_boxes = df_test['Location'].tolist()\n",
    "\n",
    "    center_format_boxes = [convert_to_center_format(box) for box in pred_boxes]\n",
    "    pred_boxes_int = [[int(coordinate) for coordinate in box] for box in center_format_boxes]\n",
    "\n",
    "    matches = 0\n",
    "\n",
    "    for pred_box in pred_boxes_int:\n",
    "        best_iou = 0\n",
    "        for gt_box in gt_boxes:\n",
    "            current_iou = iou(pred_box, gt_box)\n",
    "            best_iou = max(best_iou, current_iou)\n",
    "\n",
    "        if best_iou > iou_threshold:\n",
    "            matches += 1\n",
    "\n",
    "    precision = matches / len(pred_boxes_int) if pred_boxes_int else 0\n",
    "    recall = matches / len(gt_boxes) if gt_boxes else 0\n",
    "\n",
    "    return precision, recall\n",
    "image_ids = range(len(dataset_test))  # \n",
    "all_precisions = []\n",
    "all_recalls = []\n",
    "\n",
    "for image_id in image_ids:\n",
    "    if image_id in [12, 13, 20]:\n",
    "        continue\n",
    "    precision, recall = compute_precision_recall_for_image(image_id)\n",
    "    all_precisions.append(precision)\n",
    "    all_recalls.append(recall)\n",
    "    # \n",
    "    print(f\"Processed Image ID: {dataset_test[image_id]['image_id']}\")\n",
    "print(f\"All Precisions: {all_precisions}\")\n",
    "print(f\"All Recalls: {all_recalls}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91613daf-ce05-4879-8a86-cea43908398b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_f1_scores = []\n",
    "for precision, recall in zip(all_precisions, all_recalls):\n",
    "    if precision + recall != 0:\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        f1 = 0\n",
    "    all_f1_scores.append(f1)\n",
    "\n",
    "print(f\"All F1 Scores: {all_f1_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e2654e-0eb7-4b8a-ba10-b50c1790905e",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_results(all_f1_scores, all_precisions, all_recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0ce3f9-7f96-430d-bc84-baf4fe7b6854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(box1, box2):\n",
    "    x1, y1, w1, h1 = box1\n",
    "    x2, y2, w2, h2 = box2\n",
    "    \n",
    "    xi1 = max(x1, x2)\n",
    "    yi1 = max(y1, y2)\n",
    "    xi2 = min(x1 + w1, x2 + w2)\n",
    "    yi2 = min(y1 + h1, y2 + h2)\n",
    "    \n",
    "    intersection_area = max(xi2 - xi1, 0) * max(yi2 - yi1, 0)\n",
    "    box1_area = w1 * h1\n",
    "    box2_area = w2 * h2\n",
    "    union_area = box1_area + box2_area - intersection_area\n",
    "    \n",
    "    return intersection_area / union_area\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4f88d2-5c60-4ceb-b8c0-bf71752aff45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 打开文件并读取其内容\n",
    "with open(test_annotations_file, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# 使用data变量处理JSON内容，例如提取annotations中image_id为0的所有bbox\n",
    "gt_boxes = [list(map(round, ann[\"bbox\"])) for ann in data[\"annotations\"] if ann[\"image_id\"] == 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf2e6f3-7cdf-4314-84cd-bf304a132cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_boxes = df_test['Location'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5db218-54fe-4bfb-bfe6-f1403beaf18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_center_format(box):\n",
    "    xmin, ymin, xmax, ymax = box\n",
    "    x_center = xmin\n",
    "    y_center = ymin\n",
    "    width = xmax - xmin\n",
    "    height = ymax - ymin\n",
    "    return [x_center, y_center, width, height]\n",
    "\n",
    "center_format_boxes = [convert_to_center_format(box) for box in pred_boxes]\n",
    "pred_boxes_int = [[int(coordinate) for coordinate in box] for box in center_format_boxes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dfd0e5-ccca-4ce4-8cf4-09f9bbf41fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_threshold = 0.3\n",
    "matches = 0\n",
    "\n",
    "for pred_box in pred_boxes_int:\n",
    "    best_iou = 0\n",
    "    for gt_box in gt_boxes:\n",
    "        current_iou = iou(pred_box, gt_box)\n",
    "        best_iou = max(best_iou, current_iou)\n",
    "    \n",
    "    if best_iou > iou_threshold:\n",
    "        matches += 1\n",
    "\n",
    "precision = matches / len(pred_boxes_int)\n",
    "recall = matches / len(gt_boxes)\n",
    "\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d383a66-35b4-49f5-849e-68cb8d1fcfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 读取图片\n",
    "image_path = r\"C:\\Users\\Aliphraim\\Desktop\\edge_feature\\federico\\ML_piplie\\yolo_mini_yizhe\\notebooks\\data\\real_data\\real_test_nodes\\images\\0510e541-0f349bf1-erythrocytes_L1_2.jpg\"\n",
    "image = cv2.imread(image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # \n",
    "\n",
    "# 绘制 gt_boxes\n",
    "for box in gt_boxes:\n",
    "    x, y, w, h = box\n",
    "    cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)  # \n",
    "\n",
    "# 绘制 pred_boxes\n",
    "for box in pred_boxes_int:\n",
    "    x, y, w, h = [int(i) for i in box]\n",
    "    cv2.rectangle(image, (x, y), (x + w, y + h), (255, 0, 0), 2)  # \n",
    "\n",
    "# 使用matplotlib显示图片\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906207d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_f1 = []\n",
    "nodes_pr = []\n",
    "nodes_re = []\n",
    "\n",
    "\n",
    "\n",
    "for index in range(n_examples):\n",
    "\n",
    "    example_folder = ann_examples[index]\n",
    "    \n",
    "    if example_folder in to_exclude_to_time_complexity:\n",
    "        continue\n",
    "    \n",
    "    print(example_folder)\n",
    "\n",
    "    ann_edges = pd.read_excel(os.path.join(main_folder, example_folder, 'annotated edges.xlsx'))\n",
    "    \n",
    "    ann_nodes = pd.read_excel(os.path.join(main_folder, example_folder, 'annotated nodes.xlsx'))\n",
    "    ann_nodes = filter_image_nodes_from_annotated_df(ann_nodes)\n",
    "    \n",
    "    \n",
    "    ocr = get_ocr_data(os.path.join(main_folder, example_folder, example_folder, 'analyzeDocResponse.json'))\n",
    "\n",
    "    image = []\n",
    "    \n",
    "    image_file_name = [f for f in os.listdir(os.path.join(main_folder, example_folder)) \n",
    "                        if f.lower().endswith('png') or f.lower().endswith('jpg')][0]\n",
    "    \n",
    "    image.append(open_image(os.path.join(main_folder, example_folder,image_file_name)))\n",
    "\n",
    "    \n",
    "    image.append(threshold_image(image[-1]))\n",
    "    \n",
    "    ocr = set_bounding_boxes_in_pixels(ocr, image[-1])\n",
    "    ocr = get_font_size(ocr)\n",
    "    ocr = join_close_nodes(ocr, vertical_distance_threshold_percent=25, horizontal_distance_threshold_percent=50)\n",
    "    \n",
    "    image.append(stamp_bounding_boxes_on_image(ocr, image[-1], erotion_percent = 10))\n",
    "    #image.append(close_shape_gaps5(image[-1], ocr, dist_threshold_percent = 30))\n",
    "    \n",
    "    image.append(get_filled_shapes(image[-1]))\n",
    "    \n",
    "    nodes_mask, edges_mask = get_masks(image[-1], max_iter=5)\n",
    "    \n",
    "    image.append(nodes_mask)\n",
    "    image.append(edges_mask)\n",
    "    \n",
    "    edges_endpoints = get_edges_endpoints(edges_mask, min_edge_length_percentage=1.5)\n",
    "    \n",
    "    nodes_df = get_nodes(ocr, nodes_mask, threshold_iou = 0.3)\n",
    "    \n",
    "    nodes_df['text'] = nodes_df.text.apply(spellcheck2)\n",
    "\n",
    "    connections_df = get_conections(nodes_df, edges_endpoints, image[-1], dist_threshold_percentage = 20)\n",
    "\n",
    "    f1, pr, re = get_nodes_metrics(ann_nodes, nodes_df)\n",
    "    \n",
    "    nodes_f1.append(f1)\n",
    "    nodes_pr.append(pr)\n",
    "    nodes_re.append(re)\n",
    "    \n",
    "\n",
    "show_results(nodes_f1, nodes_pr, nodes_re)\n",
    "nodes_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb7c9c6-1528-4af8-9a5e-651e6e48f136",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd846c0d-e74b-4309-8276-8ad200c4ac8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
